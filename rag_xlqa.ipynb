{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOgPoJ447DUx"
      },
      "source": [
        "## Setup libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYRdtCdQ7DUz",
        "outputId": "7decfe3e-8863-4e02-8f46-7fe8b2816053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --upgade\n",
            "Requirement already satisfied: nuclia in /usr/local/lib/python3.10/dist-packages (1.1.19)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.336)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.17.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from nuclia) (0.5.0)\n",
            "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from nuclia) (1.10.13)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from nuclia) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from nuclia) (2.31.0)\n",
            "Requirement already satisfied: prompt-toolkit in /usr/local/lib/python3.10/dist-packages (from nuclia) (3.0.39)\n",
            "Requirement already satisfied: nucliadb-sdk>=2.30.0.post881 in /usr/local/lib/python3.10/dist-packages (from nuclia) (2.32.0.post895)\n",
            "Requirement already satisfied: nucliadb-models>=2.30.0.post881 in /usr/local/lib/python3.10/dist-packages (from nuclia) (2.32.0.post895)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nuclia) (4.66.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.64)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: nucliadb-protos>=2.32.0-post895 in /usr/local/lib/python3.10/dist-packages (from nucliadb-models>=2.30.0.post881->nuclia) (2.32.0.post895)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from nucliadb-sdk>=2.30.0.post881->nuclia) (0.25.1)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (from nucliadb-sdk>=2.30.0.post881->nuclia) (3.9.10)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0->nuclia) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->nuclia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->nuclia) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->nuclia) (2.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit->nuclia) (0.2.9)\n",
            "Requirement already satisfied: protobuf<5,>=4.22.3 in /usr/local/lib/python3.10/dist-packages (from nucliadb-protos>=2.32.0-post895->nucliadb-models>=2.30.0.post881->nuclia) (4.25.1)\n",
            "Requirement already satisfied: grpcio>=1.44.0 in /usr/local/lib/python3.10/dist-packages (from nucliadb-protos>=2.32.0-post895->nucliadb-models>=2.30.0.post881->nuclia) (1.59.2)\n",
            "Requirement already satisfied: grpcio-tools>=1.44.0 in /usr/local/lib/python3.10/dist-packages (from nucliadb-protos>=2.32.0-post895->nucliadb-models>=2.30.0.post881->nuclia) (1.59.2)\n",
            "Requirement already satisfied: mypy-protobuf>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from nucliadb-protos>=2.32.0-post895->nucliadb-models>=2.30.0.post881->nuclia) (3.5.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx->nucliadb-sdk>=2.30.0.post881->nuclia) (1.0.2)\n",
            "Requirement already satisfied: types-protobuf>=4.23.0.2 in /usr/local/lib/python3.10/dist-packages (from mypy-protobuf>=3.4.0->nucliadb-protos>=2.32.0-post895->nucliadb-models>=2.30.0.post881->nuclia) (4.24.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx->nucliadb-sdk>=2.30.0.post881->nuclia) (0.14.0)\n",
            "2023-11-16 11:39:53.995082: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-16 11:39:53.995147: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-16 11:39:53.995190: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-16 11:39:54.011099: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-16 11:39:55.740486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-16 11:39:58.182731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 11:39:58.183299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 11:39:58.183524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "!pip install --upgade pip\n",
        "!pip install nuclia langchain pypdf spacy pandas\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxyQ8h7z7DU0",
        "outputId": "b573d3ed-4e79-4e92-c7c5-e351440beba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.41.2.post2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.3)\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: feedparser==6.0.10 in /usr/local/lib/python3.10/dist-packages (from arxiv) (6.0.10)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers bitsandbytes accelerate scipy arxiv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58LrLOou7DU1"
      },
      "source": [
        "## Download and parse data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X_GGsHOT7DU1",
        "outputId": "c0c997b2-8b32-4662-ed9f-672c2b7cecbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./bert.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# donwload data\n",
        "import arxiv\n",
        "\n",
        "bert_id = \"1810.04805\"\n",
        "\n",
        "paper = next(arxiv.Client().results(arxiv.Search(id_list=[bert_id])))\n",
        "# Download the PDF to the PWD with a default filename.\n",
        "paper.download_pdf(filename=\"bert.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6y5QWsT7DU1",
        "outputId": "c493fb33-c40d-4eac-d81d-ec619635cbad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT , which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce ﬁne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based andﬁne-tuning . The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speciﬁc architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The ﬁne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciﬁc parameters, and is trained on the\\ndownstream tasks by simply ﬁne-tuning allpre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the ﬁne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying ﬁne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\nIn this paper, we improve the ﬁne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations from Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a “masked lan-\\nguage model” (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019',\n",
              " 'word based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\ntrain a deep bidirectional Transformer. In addi-\\ntion to the masked language model, we also use\\na “next sentence prediction” task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n• We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n• We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level andtoken-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert .\\n2 Related Work\\nThere is a long history of pre-training general lan-\\nguage representations, and we brieﬂy review the\\nmost widely-used approaches in this section.\\n2.1 Unsupervised Feature-based Approaches\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods. Pre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signiﬁcant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).These approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speciﬁc architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n2.2 Unsupervised Fine-tuning Approaches\\nAs with the feature-based approaches, the ﬁrst\\nworks in this direction only pre-trained word em-\\nbedding parameters from unlabeled text (Col-\\nlobert and Weston, 2008).']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Parse data from PDFs\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import SpacyTextSplitter\n",
        "\n",
        "\n",
        "file_name = \"bert.pdf\"\n",
        "loader = PyPDFLoader(file_name)\n",
        "\n",
        "# explore the content extracted\n",
        "content_pages = [page.page_content for page in loader.load_and_split()]\n",
        "content_pages[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nnnl9nZ7DU1"
      },
      "source": [
        "## Process data from PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07dKKiFP7DU1",
        "outputId": "504ffd89-b8a0-49fb-ad1d-930f3f4e9a0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT , which stands for\\nBidirectional Encoder Representations from\\nTransformers.\\n\\nUnlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers.\\n\\nAs a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\n\\n\\nBERT is conceptually simple and empirically\\npowerful.',\n",
              " 'BERT is conceptually simple and empirically\\npowerful.\\n\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018).']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from langchain.text_splitter import SpacyTextSplitter\n",
        "text_splitter = SpacyTextSplitter(chunk_size=1000)\n",
        "\n",
        "# create chunks of maximum length 100\n",
        "content_chunks = []\n",
        "for content_pages in content_pages:\n",
        "  texts = text_splitter.split_text(content_pages)\n",
        "  content_chunks.extend(texts)\n",
        "\n",
        "content_chunks[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpKxoMGX7DU1"
      },
      "source": [
        "## Use NucliaDB vector database to upload data and retrieve contexts\n",
        "\n",
        "Follow the LangChain documentation to use NucliaDB:\n",
        "\n",
        "- https://python.langchain.com/docs/integrations/vectorstores/nucliadb\n",
        "- https://nuclia.com/developers/using-langchain-with-nuclia/\n",
        "\n",
        "Alternatively, you can directly use NucliaDB:\n",
        "- https://nuclia.com/nucliadb/nucliadb-vector-database-data-scientists/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdr8Y15C7DU2",
        "outputId": "60d53a78-b4a9-4a4d-a712-d672086c0ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid service token\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores.nucliadb import NucliaDB\n",
        "\n",
        "API_KEY = \"YOUR_API_KEY\"\n",
        "KB_ID = \"YOUR_KB_ID\"\n",
        "ndb = NucliaDB(knowledge_box=KB_ID, local=False, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNpIzs587DU2"
      },
      "outputs": [],
      "source": [
        "# Upload data\n",
        "# Note that the BERT paper has already been uploaded, uncomment the line below to upload new data\n",
        "content_chunks_ids = ndb.add_texts(content_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "b-woCwwC7DU2",
        "outputId": "126ba2c2-5e85-4d81-d331-054e4e3b5a8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We use a dropout probability of 0.1 on all layers. We use a gelu activation (Hendrycks and Gimpel, 2016) rather than the standard relu , following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood. Training of BERT BASE was performed on 4 Cloud TPUs in Pod conﬁguration (16 TPU chips total).13Training of BERT LARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pretraining took 4 days to complete. Longer sequences are disproportionately expensive because attention is quadratic to the sequence length. To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings.\\n\\n- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations. BERT is conceptually simple and empirically powerful.\\n\\n- Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the ofﬁcial GLUE leaderboard10, BERT LARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing. We ﬁnd that BERT LARGE signiﬁcantly outperforms BERT BASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2. 4.2 SQuAD v1.1 The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowdsourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from 9The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT BASE and BERT LARGE .\\n\\n- System Dev Test EM F1 EM F1 Top Leaderboard Systems (Dec 10th, 2018) Human 82.3 91.2 #1 Ensemble nlnet 86.0 91.7 #2 Ensemble QANet 84.5 90.5 Published BiDAF+ELMo (Single) 85.6 85.8 R.M. Reader (Ensemble) 81.2 87.9 82.3 88.5 Ours BERT BASE (Single) 80.8 88.5 BERT LARGE (Single) 84.1 90.9 BERT LARGE (Ensemble) 85.8 91.8 BERT LARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8 BERT LARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2 Table 2: SQuAD 1.1 results. The BERT ensemble is 7x systems which use different pre-training checkpoints and ﬁne-tuning seeds. System Dev Test EM F1 EM F1 Top Leaderboard Systems (Dec 10th, 2018) Human 86.3 89.0 86.9 89.5 #1 Single MIR-MRC (F-Net) 74.8 78.0 #2 Single nlnet 74.2 77.1 Published unet (Ensemble) 71.4 74.9 SLQA+ (Single) 71.4 74.4 Ours BERT LARGE (Single) 78.7 81.9 80.0 83.1 Table 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Retrieve questions\n",
        "question = \"What does BERT mean?\"\n",
        "def retrieve(question, topk=5):\n",
        "    retrieved_contexts = [result.page_content for result in ndb.similarity_search(query=question, k=topk)]\n",
        "    # clean a bit\n",
        "    retrieved_contexts = [context.replace(\"- \", \"\").replace(\"\\n\", \" \").replace(\"  \", \" \").strip() for context in retrieved_contexts]\n",
        "    return \"\\n\\n- \".join(retrieved_contexts)\n",
        "retrieve(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB5ElFfJ7DU2"
      },
      "source": [
        "## Use Transformers library to use the Mistral-instruct model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "954269ddc98d4da6a2bd5ca2f36faa47",
            "078ad35eeada498e91a856d8876eada6",
            "fde57502e3c84f37b0492c774dc22f5b",
            "55204da7f7c54e7586f38df360dc55f7",
            "d94fb9f7192541a29281572c92701456",
            "e2ae95ba18df4219a35eeac7caacf6ad",
            "cefcbdc50cd844c9ba0126565f1c7035",
            "9fbc56df77f44affb69c12218124b8f1",
            "98f47a5490d94f5384d50ef1856e1d65",
            "b89043c493294ebc9ca98e306477ce30",
            "8147e63c116b459ebacee3822916191d"
          ]
        },
        "id": "QTa0bEzg7DU2",
        "outputId": "ccd8cf12-acbc-40be-b8e6-67ef129aecbf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "954269ddc98d4da6a2bd5ca2f36faa47"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "MISTRAL_MODEL = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "model = AutoModelForCausalLM.from_pretrained(MISTRAL_MODEL, quantization_config=nf4_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MISTRAL_MODEL)\n",
        "\n",
        "generation_args = {\n",
        "            \"max_new_tokens\": 1024,\n",
        "            \"do_sample\": True,\n",
        "            \"top_k\": 10,\n",
        "            \"top_p\": 0.9,\n",
        "            \"eos_token_id\": tokenizer.eos_token_id,\n",
        "            \"batch_size\": 1,\n",
        "        }\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            **generation_args\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xifmD73I7DU3"
      },
      "source": [
        "## Create a RAG QA system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u11dyINx7DU3",
        "outputId": "d7e9af39-4c8c-40bf-8e8c-61b2d2762c29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"<s>[INST] <<SYS>>\\nNLP assistant\\n<</SYS>>\\n\\nWhat's your name? [/INST]\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_prompt(\n",
        "    system_prompt: str,\n",
        "    message: str,\n",
        ") -> str:\n",
        "    texts = [f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n\"]\n",
        "    # prepend chat history before the current message\n",
        "    do_strip = False\n",
        "    message = message.strip() if do_strip else message\n",
        "    texts.append(f\"{message} [/INST]\")\n",
        "    return \"\".join(texts)\n",
        "\n",
        "get_prompt(\"NLP assistant\", \"What's your name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts9Nzaa07DU3"
      },
      "outputs": [],
      "source": [
        "def rag_xlqa(system_prompt, test_question, top_k):\n",
        "\n",
        "    # retrieve contexts with semantic search\n",
        "    context = retrieve(test_question, topk=top_k)\n",
        "    if not context:\n",
        "        context = \"There is no context available for the given question.\"\n",
        "\n",
        "    # create user and assistant message applying a QA task template with context as input\n",
        "    user_msg = f\"Input\\n{context}\\nQ: {test_question}\"\n",
        "\n",
        "    # format messages using chat template to build model-specific\n",
        "    prompt = get_prompt(system_prompt, user_msg)\n",
        "    response = pipeline(prompt)\n",
        "\n",
        "    # answer by taking the last message after the user message\n",
        "    answer = response[0][\"generated_text\"].split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    return response, answer, context\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4T3RxKg7DU3",
        "outputId": "d1ca58ac-c8b0-4768-c7d7-37332f6d4169"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('Not enough information to answer.',\n",
              " 'There is no context available for the given question.')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SYSTEM_PROMPT_XLQA = \"Use the following pieces of context to answer the question at the end.\\n\"\\\n",
        "                     \"If there is not enough information in the context don't try to make up an answer, just say 'Not enough information to answer'.\\n\"\\\n",
        "                    \"Just provide one response and do not continue the response.\"\\\n",
        "                     \"Answer in the language of the question.\"\n",
        "TEST_QUESTION = \"Who is Einstein?\"\n",
        "response, answer, context = rag_xlqa(system_prompt=SYSTEM_PROMPT_XLQA, test_question=TEST_QUESTION, top_k=10)\n",
        "answer, context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGU4kJzJ7DU3"
      },
      "source": [
        "## Testing cross-lingual QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGKtShsF7DU3"
      },
      "outputs": [],
      "source": [
        "questions_bert_paper = {\n",
        "    'english': [\n",
        "        \"What specific pre-training techniques were employed in the BERT model?\",\n",
        "        \"How many epochs were conducted during the pre-training phase of BERT?\",\n",
        "        \"Could you elaborate on the tokenization strategy adopted for the BERT model?\",\n",
        "        \"What is the size of the vocabulary?\",\n",
        "        \"What evaluation metrics were employed to assess the performance of BERT?\",\n",
        "        \"Can you specify the size of the training dataset used for pre-training BERT?\",\n",
        "        \"What were the key findings or improvements observed in the experiments conducted with the BERT model?\"\n",
        "    ],\n",
        "    'spanish': [\n",
        "        \"¿Qué técnicas específicas de preentrenamiento se utilizaron en el modelo BERT?\",\n",
        "        \"¿Cuántas épocas se llevaron a cabo durante la fase de preentrenamiento de BERT?\",\n",
        "        \"¿Podría detallar la estrategia de tokenización adoptada para el modelo BERT?\",\n",
        "        \"¿Cuál es el tamaño del vocabulario?\",\n",
        "        \"¿Qué métricas de evaluación se emplearon para evaluar el rendimiento de BERT?\",\n",
        "        \"¿Puede especificar el tamaño del conjunto de datos de entrenamiento utilizado para el preentrenamiento de BERT?\",\n",
        "        \"¿Cuáles fueron los hallazgos clave o mejoras observadas en los experimentos realizados con el modelo BERT?\"\n",
        "    ],\n",
        "    'catalan': [\n",
        "        \"Quines tècniques específiques de preentrenament es van emprar en el model BERT?\",\n",
        "        \"Quantes èpoques es van dur a terme durant la fase de preentrenament de BERT?\",\n",
        "        \"Podria donar detalls sobre la estratègia de tokenització adoptada per al model BERT?\",\n",
        "        \"Quin és el tamany del vocabulari?\",\n",
        "        \"Quines mètriques d'avaluació es van utilitzar per avaluar el rendiment de BERT?\",\n",
        "        \"Podria especificar el tamany del conjunt de dades d'entrenament utilitzat per al preentrenament de BERT?\",\n",
        "        \"Quins van ser els resultats clau o millores observades en els experiments realitzats amb el model BERT?\"\n",
        "    ],\n",
        "    \"italian\": [\n",
        "        \"Quali specifiche tecniche di pre-training sono state impiegate nel modello BERT?\",\n",
        "        \"Quante epoche sono state condotte durante la fase di pre-training di BERT?\",\n",
        "        \"Potresti approfondire sulla strategia di tokenizzazione adottata per il modello BERT?\",\n",
        "        \"Qual è la dimensione del vocabolario?\",\n",
        "        \"Quali metriche di valutazione sono state impiegate per valutare le performance di BERT?\",\n",
        "        \"Puoi specificare la dimensione del set di dati di addestramento utilizzato per il pre-training di BERT?\",\n",
        "        \"Quali sono stati i principali risultati o miglioramenti osservati negli esperimenti condotti con il modello BERT?\"\n",
        "    ],\n",
        "    \"japanese\": [\n",
        "        \"BERTモデルで具体的にどのような事前トレーニング技術が使用されましたか？\",\n",
        "        \"BERTの事前トレーニングフェーズで何エポックが行われましたか？\",\n",
        "        \"BERTモデルで採用されたトークナイゼーション戦略について詳しく説明できますか？\",\n",
        "        \"ボキャブラリーのサイズはどれくらいですか？\",\n",
        "        \"BERTのパフォーマンスを評価するためにどの評価メトリクスが使用されましたか？\",\n",
        "        \"BERTの事前トレーニングに使用されたトレーニングデータセットのサイズを指定できますか？\",\n",
        "        \"BERTモデルで行われた実験で観察された主な結果や改善点は何でしたか？\"\n",
        "    ],\n",
        "    'telugu': [\n",
        "        \"BERT మోడల్‌లో ఏదైనా క్రియాశీల ప్రీట్రైనింగ్ విధులు ఉపయోగించబడిందివా?\",\n",
        "        \"BERT ప్రీట్రైనింగ్ దశలో ఎంత ఎపాక్స్ నడిపబడింది?\",\n",
        "        \"BERT మోడల్‌కు అంగీకృతమయ్యే టోకెనైజేషన్ స్ట్రాటజీపై మీరు వివరాలు ఇవ్వగలరా?\",\n",
        "        \"వాకార్గ్రాం పరిమాణమేంటి?\",\n",
        "        \"BERT యొక్క ప్రదర్శనను అంగీకరించడానికి ఏమిటి అనుసరించిన అంశాలు?\",\n",
        "        \"BERT ప్రీట్రైనింగ్ కోసం ఉపయోగించిన ప్రశిక్షణ డేటాసెట్ పరిమాణాన్ని నిర్దిష్టంగా చెప్పగలరా?\",\n",
        "        \"BERT మోడల్‌తో నడుపబడిన ప్రయోగాలలో ఏమిటి ప్రధాన ఫైండింగ్‌లు లేదా మెరుగుపరచిన మెరుగులు?\"\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT1eNKAU7DU4",
        "outputId": "5e96b963-fa5c-46a9-e1a9-7e6450fc06b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/home/learning/.pyenv/versions/3.9.15/envs/crosslingual_document_search/lib/python3.9/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "SYSTEM_PROMPT_XLQA = \"You are an NLP assistant whose purpose is to solve reading comprehension problems.\\n\"\\\n",
        "                     \"You will be provided questions on a set of passages and you will need to provide the answer as it appears in the passage.\\n\"\\\n",
        "                     \"If there is not enough information in the context don't try to make up an answer, just say 'Not enough information to answer'.\\n\"\\\n",
        "                     \"Just provide one response and do not continue the response.\"\\\n",
        "                     \"Let's think step-by-step and answer in the language of the question.\"\n",
        "\n",
        "results = []\n",
        "for lang in questions_bert_paper:\n",
        "    qid = 0\n",
        "    for test_question in questions_bert_paper[lang]:\n",
        "        response, answer, context = rag_xlqa(system_prompt=SYSTEM_PROMPT_XLQA, test_question=test_question, top_k=20)\n",
        "        result = {\"qid\": qid, \"lang\":lang, \"question\": test_question, \"answer\":answer,\"context\": context}\n",
        "        results.append(result)\n",
        "        qid += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSrPwN5n7DU4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "954269ddc98d4da6a2bd5ca2f36faa47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_078ad35eeada498e91a856d8876eada6",
              "IPY_MODEL_fde57502e3c84f37b0492c774dc22f5b",
              "IPY_MODEL_55204da7f7c54e7586f38df360dc55f7"
            ],
            "layout": "IPY_MODEL_d94fb9f7192541a29281572c92701456"
          }
        },
        "078ad35eeada498e91a856d8876eada6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2ae95ba18df4219a35eeac7caacf6ad",
            "placeholder": "​",
            "style": "IPY_MODEL_cefcbdc50cd844c9ba0126565f1c7035",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "fde57502e3c84f37b0492c774dc22f5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fbc56df77f44affb69c12218124b8f1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98f47a5490d94f5384d50ef1856e1d65",
            "value": 0
          }
        },
        "55204da7f7c54e7586f38df360dc55f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b89043c493294ebc9ca98e306477ce30",
            "placeholder": "​",
            "style": "IPY_MODEL_8147e63c116b459ebacee3822916191d",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "d94fb9f7192541a29281572c92701456": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2ae95ba18df4219a35eeac7caacf6ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cefcbdc50cd844c9ba0126565f1c7035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fbc56df77f44affb69c12218124b8f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98f47a5490d94f5384d50ef1856e1d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b89043c493294ebc9ca98e306477ce30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8147e63c116b459ebacee3822916191d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}